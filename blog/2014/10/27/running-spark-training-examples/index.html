<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
  <meta charset="utf-8">
  <title>Running Examples From Databricks Spark Tutorial - Data Analytics on SoftLayer</title>
  <meta name="author" content="Irina Fedulova">

  
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://irifed.github.io/vagrant-cluster/blog/2014/10/27/running-spark-training-examples">
  <link href="/vagrant-cluster/favicon.png" type="image/png" rel="icon">
  <link href="/vagrant-cluster/atom.xml" rel="alternate" title="Data Analytics on SoftLayer" type="application/atom+xml">

  <!-- http://opengraphprotocol.org/ -->
  <meta name="twitter:card" content="summary_large_image">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://irifed.github.io/vagrant-cluster/blog/2014/10/27/running-spark-training-examples">
  <meta property="og:title" content="Running Examples From Databricks Spark Tutorial - Data Analytics on SoftLayer">
  

  <script src="/vagrant-cluster/javascripts/libs/jquery/jquery-2.0.3.min.js"></script>

<link href="/vagrant-cluster/assets/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/vagrant-cluster/assets/bootstrap/dist/css/bootstrap-theme.min.css" rel="stylesheet" type="text/css">


  
  <link href="/vagrant-cluster/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">

  

</head>

  <body   >
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="wrap">
      <header role="banner">
        <nav class="navbar navbar-default" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/vagrant-cluster/">Data Analytics on SoftLayer</a>
        </div>

        <div class="navbar-collapse collapse">
            <ul class="nav navbar-nav">
                <li class="active">
                    <a rel="index" href="/vagrant-cluster/">Blog</a>
                </li>
                <li >
                    <a href="/vagrant-cluster/blog/archives">Archives</a>
                </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a class="subscribe-rss" href="/vagrant-cluster/atom.xml" title="subscribe via RSS">
                        <span class="visible-xs">RSS</span>
                        <img class="hidden-xs" src="/vagrant-cluster/images/rss.png" alt="RSS">
                    </a>
                </li>
                
            </ul>
            
                <form class="search navbar-form navbar-right" action="https://www.google.com/search" method="GET">
                    <input type="hidden" name="q" value="site:irifed.github.io/vagrant-cluster">
                    <div class="form-group">
                        <input class="form-control" type="text" name="q" placeholder="Search">
                    </div>
                </form>
            
        </div>
    </div>
</nav>


      </header>
      <div id="main" role="main" class="container">
        <div id="content">
          <div class="row">
  <div class="page-content col-md-9" itemscope itemtype="http://schema.org/Blog">
    <meta itemprop="name" content="Data Analytics on SoftLayer" />
    
    <meta itemprop="url" content="http://irifed.github.io/vagrant-cluster" />
    <article class="hentry" role="article" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
      
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        












<span class="glyphicon glyphicon-calendar"></span> <time datetime="2014-10-27T12:55:04+03:00"  data-updated="true" itemprop="datePublished dateCreated">2014-10-27</time>
        
      </p>
    
    
    <h1 class="entry-title" itemprop="name headline">
        Running Examples From Databricks Spark Tutorial
        
    </h1>
    
  </header>


<div class="entry-content clearfix" itemprop="articleBody description"><p>In this post we show how to run hands-on excercises from <a href="https://databricks-training.s3.amazonaws.com/index.html">Spark Training</a> tutorial prepared by <a href="http://databricks.com/">Databricks</a> on the data analytics cluster provisioned by Vagrant-Cluster tool. Note that cluster provisioning and setup using this tool is described in detail <a href="https://github.com/irifed/vagrant-cluster">here</a>.</p>

<p>Originally Spark Training examples are prepared for running on a local Spark deployment. To run these examples on our cluster on SoftLayer we will have to make some minor modifications, e.g. use different parameters for <code>spark-submit</code>. Here we cover only differences between what was suggested in original Databricks tutorial and what should be done to run examples on the cluster. For detailed description of examples we strongly encourage you to refer to the original <a href="https://databricks-training.s3.amazonaws.com/index.html">Spark Training</a> tutorial.</p>

<h2>Logging in to the cluster</h2>

<p>Assuming that you used <code>vagrant-cluster</code> for cluster provisioning, you are on your local machine in the directory with <code>Vagrantfile</code> of your cluster. Start with logging in to master server:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>user@laptop vagrant-cluster$ ls
</span><span class='line'>LICENSE
</span><span class='line'>README.md
</span><span class='line'>Vagrantfile
</span><span class='line'>ansible-bdas
</span><span class='line'>ansible.cfg
</span><span class='line'>sl_config.yml
</span><span class='line'>sl_config.yml.template
</span><span class='line'>
</span><span class='line'>user@laptop vagrant-cluster$ vagrant ssh master
</span><span class='line'>
</span><span class='line'>Welcome to Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-36-generic x86_64)
</span><span class='line'>
</span><span class='line'> * Documentation:  https://help.ubuntu.com/
</span><span class='line'>root@master:~#</span></code></pre></td></tr></table></div></figure>


<h2>Running Sample Application</h2>

<p>Download Spark application templates and input data for the Databricks tutorial:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# wget https://s3-us-west-2.amazonaws.com/databricks-meng/usb.zip
</span><span class='line'>root@master:~# unzip usb.zip</span></code></pre></td></tr></table></div></figure>


<p>Our first task will be to verify that our Spark cluster is working by running a sample application:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# cd spark-training/simple-app</span></code></pre></td></tr></table></div></figure>


<p>In the Databricks tutorial all examples in <code>spark-training</code> folder are set up to run from local Spark deployment (no cluster) and take input files from local filesystem by default. But here we have a real Spark cluster with HDFS, so we should put our input data to HDFS to be accessible by workers.</p>

<p>For example to run SampleApp we should put some file, e.g. <code>README.md</code> file to HDFS:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# hadoop fs -put ../spark/README.md /</span></code></pre></td></tr></table></div></figure>


<p>and make corresponding changes to <code>src/main/SimpleApp.scala</code> by adding <code>hdfs:///</code> prefix to input file location, so that <code>SimpleApp.scala</code> file looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>...
</span><span class='line'>val logFile = "hdfs:///README.md"
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>Compile and run SimpleApp:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# sbt assembly
</span><span class='line'>root@master:~# /opt/spark/bin/spark-submit --class "SimpleApp" \ 
</span><span class='line'>    --master spark://master:7077 target/scala-2.10/simple-project_2.10-1.0.jar
</span><span class='line'>...
</span><span class='line'>Lines with a: 83, Lines with b: 38</span></code></pre></td></tr></table></div></figure>


<p>Note that we are running <code>sbt</code> without any prefix paths because SBT is installed system-wide on your cluster. Also we submit SimpleApp to the real Spark cluster instead of local mode using <code>--master spark://master:7077</code> option.</p>

<h2>Data exploration using Spark SQL</h2>

<p>Next, move to the next page of the Databricks tutorial and try running some <a href="https://databricks-training.s3.amazonaws.com/data-exploration-using-spark-sql.html">Spark SQL examples</a>. We will again need to do some tweaks to run examples from Databrick tutorial on our cluster. Namely, Spark SQL section of this tutorial uses input data stored in <code>spark-training/data</code> folder on local disk, but we are going to run on a real cluster, so we should put data to HDFS:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# cd /root/spark-training
</span><span class='line'>root@master:~# hadoop fs -put data /root/data</span></code></pre></td></tr></table></div></figure>


<p>Now you can run all examples from above mentioned <a href="https://databricks-training.s3.amazonaws.com/data-exploration-using-spark-sql.html">Spark SQL</a> section of Databricks Spark Training tutorial without modifications.</p>

<h2>Stream processing with Spark Streaming</h2>

<p>To run examples from <a href="https://databricks-training.s3.amazonaws.com/realtime-processing-with-spark-streaming.html">Spark Streaming</a> section of the tutorial we again need to do only few modifications. To compile and submit your Spark Streaming application use following commands instead of those suggested in the tutorial:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# sbt assembly
</span><span class='line'>root@master:~# /opt/spark/bin/spark-submit --master spark://master:7077 \ 
</span><span class='line'>--class "Tutorial" \
</span><span class='line'>--master spark://master:7077 target/scala-2.10/Tutorial-assembly-0.1-SNAPSHOT.jar</span></code></pre></td></tr></table></div></figure>


<p>All other contents of Spark Streaming example can be run without modifications on our cluster.</p>

<h2>Movie Recommendation with MLlib</h2>

<p>This section explains how to use MLlib to make personalized movie recommendations. Please read <a href="https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html">detailed description</a> of this example in the original Spark Training tutorial.</p>

<p>To run MLlib example on our cluster on SoftLayer we again have to make some modifications.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# cd /root/spark-training/machine-learning</span></code></pre></td></tr></table></div></figure>


<p>To make personalized movie recommendations, you should first provide your personal ratings for a few movies as described <a href="https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html#create-training-examples">here</a>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~/spark-training/machine-learning# python bin/rateMovies</span></code></pre></td></tr></table></div></figure>


<p>This script will ask you to provide numerical rating to a set of movies and write result to a file <code>personalRatings.txt</code>.</p>

<p>Move on to the directory where Scala source code for this Movie rating algorithm is stored:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~/spark-training/machine-learning# cd scala</span></code></pre></td></tr></table></div></figure>


<p>Our cluster is running Spark 1.1.0 on top of CDH 5, so original <code>build.sbt</code> file should be modified and look like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import AssemblyKeys._
</span><span class='line'>
</span><span class='line'>assemblySettings
</span><span class='line'>
</span><span class='line'>name := "movielens-als"
</span><span class='line'>
</span><span class='line'>version := "0.1"
</span><span class='line'>
</span><span class='line'>scalaVersion := "2.10.4"
</span><span class='line'>
</span><span class='line'>libraryDependencies += "org.apache.spark" %% "spark-core" % "1.1.0" % "provided"
</span><span class='line'>
</span><span class='line'>libraryDependencies += "org.apache.spark" %% "spark-mllib" % "1.1.0" % "provided"
</span><span class='line'>
</span><span class='line'>libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.3.0" % "provided"
</span><span class='line'>
</span><span class='line'>libraryDependencies += "org.apache.hadoop" % "hadoop-hdfs" % "2.3.0" % "provided"</span></code></pre></td></tr></table></div></figure>


<p>You can download this file to your cluster using following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>wget htts://TODO-TODO/build.sbt</span></code></pre></td></tr></table></div></figure>


<p>After updating <code>build.sbt</code> program can be compiled with SBT assembly command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~/spark-training/machine-learning/scala# sbt assembly</span></code></pre></td></tr></table></div></figure>


<p>To run assembled Spark application use following parameters for <code>spark-submit</code> script:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~/spark-training/machine-learning/scala# /opt/spark/bin/spark-submit \
</span><span class='line'>--master spark://master:7077 \
</span><span class='line'>--class "MovieLensALS" \
</span><span class='line'>target/scala-2.10/movielens-als-assembly-0.1.jar \
</span><span class='line'>hdfs://master:8020/movielens/large ../personalRatings.txt</span></code></pre></td></tr></table></div></figure>


<p>Note that in this command we provide Spark master address <code>spark://master:7077</code>, &ldquo;MovieLensALS&rdquo; class name and assembly jar and two command line parameters for MovieLensALS class: path to <code>movielens/large</code> dataset and path to <code>personalRatings.txt</code> file.</p>

<p>Because you are now running your program on the real Spark cluster, input dataset should be either stored on local file system accessible by all nodes at the same path, or on HDFS. Cluster provisioning script already has preloaded <code>movielens/medium</code> and <code>movielens/large</code> datasets on HDFS of your cluster, so you should provide dataset path as <code>hdfs://master:8020/movielens/medium/</code> or <code>hdfs://master:8020/movielens/large/</code>. Note trailing <code>/</code> at the end of HDFS urls, this is very important for correct constructing of path in <code>MovieLensALS.scala</code>.</p>

<p>Finally, <code>MovieLensALS.scala</code> program is desisgned to read input file from local filesystem, so you should modify a couple of lines in <code>MovieLensALS.scala</code> to read files from HDFS.
Replace line</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val ratings = sc.textFile(new File(movieLensHomeDir, "ratings.dat").toString).map { line =&gt;</span></code></pre></td></tr></table></div></figure>


<p>
by</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val ratings = sc.textFile(movieLensHomeDir + "ratings.dat").map { line =&gt;</span></code></pre></td></tr></table></div></figure>


<p>and do the same with <code>val movies = ...</code> line so it looks like</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val movies = sc.textFile(movieLensHomeDir + "movies.dat").map { line =&gt;</span></code></pre></td></tr></table></div></figure>


<p>Here we simply replaced <code>new File(movieLensHomeDir, "ratings.dat").toString</code> construct by <code>movieLensHomeDir + "ratings.dat"</code>, because <code>new File(...)</code> creates a local File instance from a parent and child path strings, but we do not need this because we provided full HDFS link as a parameter to <code>spark-submit</code>.</p>

<p>Remaining contents of <a href="https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html">MLlib example</a> does not require any modifications for running on a real cluster.</p>

<h2>Graph Analytics With GraphX</h2>

<p>Start Spark shell as follows:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/opt/spark/bin/spark-shell --master spark://master:7077</span></code></pre></td></tr></table></div></figure>


<p>The only additional step is required for <a href="https://databricks-training.s3.amazonaws.com/graph-analytics-with-graphx.html#load-the-wikipedia-articles">Wikipedia analysis</a> section of this chapter.</p>

<p>Note that Kryo serializer is already enabled by default on your cluster, so you should skip <a href="https://databricks-training.s3.amazonaws.com/graph-analytics-with-graphx.html#constructing-an-end-to-end-graph-analytics-pipeline-on-real-data">steps</a> explaining how to enable it.</p>

<p>Again, original tutorial reads data from local disk, but we should put it to HDFS:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# cd /spark-training/
</span><span class='line'>root@master:~# hadoop fs -put data/graphx /data/graphx</span></code></pre></td></tr></table></div></figure>


<p>and do not forget to use correct HDFS path (<code>/data/graphx/...</code> instead of <code>data/graphx/...</code>) when loading Wikipedia articles as RDD:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val articles: RDD[String] = sc.textFile("/data/graphx/graphx-wiki-vertices.txt")
</span><span class='line'>val links: RDD[String] = sc.textFile("/data/graphx/graphx-wiki-edges.txt")</span></code></pre></td></tr></table></div></figure>


<p>As with previous chapters, all remaining content can be run on the cluster without modifications.</p>
</div>


      <footer>
        <p class="meta text-muted">
          
  

<span class="glyphicon glyphicon-user"></span> <span class="byline author vcard" itemprop="author" itemscope itemtype="http://schema.org/Person">Posted by <span class="fn" itemprop="name">Irina Fedulova</span></span>

          












<span class="glyphicon glyphicon-calendar"></span> <time datetime="2014-10-27T12:55:04+03:00"  data-updated="true" itemprop="datePublished dateCreated">2014-10-27</time>
          


        </p>
        
          <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://irifed.github.io/vagrant-cluster/blog/2014/10/27/running-spark-training-examples/" data-via="" data-counturl="http://irifed.github.io/vagrant-cluster/blog/2014/10/27/running-spark-training-examples/" >Tweet</a>
  
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

        
        
          <ul class="meta text-muted pager">
            
            
            <li class="next"><a href="/vagrant-cluster/blog/2014/12/08/softlayer-cluster-manager/" title="Next Post: Deployment of data analytics cluster on SoftLayer">Deployment of data analytics cluster on SoftLayer &raquo;</a></li>
            
          </ul>
        
      </footer>
    </article>
    
  </div>

  
  <aside class="sidebar col-md-3">
    
      <section class="panel panel-default">
  <div class="panel-heading">
    <h3 class="panel-title">Recent Posts</h3>
  </div>
  
  <div id="recent_posts" class="list-group">
    
    <a class="list-group-item " href="/vagrant-cluster/blog/2014/12/08/softlayer-cluster-manager/">Deployment of Data Analytics Cluster on SoftLayer</a>
    
    <a class="list-group-item active" href="/vagrant-cluster/blog/2014/10/27/running-spark-training-examples/">Running Examples From Databricks Spark Tutorial</a>
    
  </div>
</section>






    
  </aside>
  
</div>

        </div>
      </div>
    </div>
    <footer role="contentinfo"><div class="container">
    <p class="text-muted credits">
  Copyright &copy; 2014 - Irina Fedulova<br>
  <small>
      <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>,
      <span class="credit">customized with <a href="https://github.com/kAworu/octostrap3">octostrap3</a></span>.
  </small>
</p>

</div>
</footer>
    



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>


<script src="/vagrant-cluster/assets/bootstrap/dist/js/bootstrap.min.js"></script>
<script src="/vagrant-cluster/javascripts/modernizr-2.0.js"></script>


  </body>
</html>
