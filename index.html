
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Data Analytics on SoftLayer</title>
  <meta name="author" content="Irina Fedulova">

  
  <meta name="description" content="In this post we show how to run hands-on excercises from Spark Training tutorial prepared by Databricks. Note that cluster provisioning and setup &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://irifed.github.io/vagrant-cluster">
  <link href="/vagrant-cluster/favicon.png" rel="icon">
  <link href="/vagrant-cluster/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/vagrant-cluster/atom.xml" rel="alternate" title="Data Analytics on SoftLayer" type="application/atom+xml">
  <script src="/vagrant-cluster/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/vagrant-cluster/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/vagrant-cluster/">Data Analytics on SoftLayer</a></h1>
  
    <h2>Usage examples of Vagrant-Cluster tool</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/vagrant-cluster/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:irifed.github.io/vagrant-cluster" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/vagrant-cluster/">Blog</a></li>
  <li><a href="/vagrant-cluster/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/vagrant-cluster/blog/2014/10/27/running-spark-training-examples/">Running Examples From Databricks Spark Tutorial</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2014-10-27T12:55:04+03:00'><span class='date'><span class='date-month'>Oct</span> <span class='date-day'>27</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>12:55 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In this post we show how to run hands-on excercises from <a href="https://databricks-training.s3.amazonaws.com/index.html">Spark Training</a> tutorial prepared by <a href="http://databricks.com/">Databricks</a>. Note that cluster provisioning and setup using <code>vagrant-cluster</code> tool is described in the <a href="TODO">previous post</a>.</p>

<p>Originally Spark Training examples are prepared for running on a local Spark deployment. To run these examples on our cluster on SoftLayer we will have to make some minor modifications, e.g. use different parameters for <code>spark-submit</code>. Here we cover only differences between what was suggested in original Databricks tutorial and what should be done to run examples on the cluster. For detailed example description we strongly encourage you to refer to the original <a href="https://databricks-training.s3.amazonaws.com/index.html">Spark Training</a> tutorial.</p>

<h2>Logging in to the cluster</h2>

<p>Assuming that you used <code>vagrant-cluster</code> for cluster provisioning, you are on your local machine in the directory with <code>Vagrantfile</code> of your cluster. Start with logging in to master server:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>user@laptop vagrant-cluster$ ls
</span><span class='line'>LICENSE
</span><span class='line'>README.md
</span><span class='line'>Vagrantfile
</span><span class='line'>ansible-bdas
</span><span class='line'>ansible.cfg
</span><span class='line'>sl_config.yml
</span><span class='line'>sl_config.yml.template
</span><span class='line'>
</span><span class='line'>user@laptop vagrant-cluster$ vagrant ssh master
</span><span class='line'>
</span><span class='line'>Welcome to Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-36-generic x86_64)
</span><span class='line'>
</span><span class='line'> * Documentation:  https://help.ubuntu.com/
</span><span class='line'>root@master:~#</span></code></pre></td></tr></table></div></figure>


<h2>Running Sample Application</h2>

<p>Download Spark application templates and input data for the Databricks tutorial:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# wget https://s3-us-west-2.amazonaws.com/databricks-meng/usb.zip
</span><span class='line'>root@master:~# unzip usb.zip</span></code></pre></td></tr></table></div></figure>


<p>Our first task will be to verify that our Spark cluster is working by running a sample application:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# cd spark-training/simple-app</span></code></pre></td></tr></table></div></figure>


<p>In the Databricks tutorial all examples in <code>spark-training</code> folder are set up to run from local Spark deployment (no cluster) and take input files from local filesystem by default. But here we have a real Spark cluster with HDFS, so we should put our input data to HDFS to be accessible by workers.</p>

<p>For example to run SampleApp we should put some file, e.g. <code>README.md</code> file to HDFS:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# hadoop fs -put ../spark/README.md /</span></code></pre></td></tr></table></div></figure>


<p>and make corresponding changes to <code>src/main/SimpleApp.scala</code> by adding <code>hdfs:///</code> prefix to input file location, so that <code>SimpleApp.scala</code> file looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>...
</span><span class='line'>val logFile = "hdfs:///README.md"
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>Compile and run SimpleApp:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# sbt package
</span><span class='line'>root@master:~# /opt/spark/bin/spark-submit --class "SimpleApp" \ 
</span><span class='line'>    --master spark://master:7077 target/scala-2.10/simple-project_2.10-1.0.jar
</span><span class='line'>...
</span><span class='line'>Lines with a: 83, Lines with b: 38</span></code></pre></td></tr></table></div></figure>


<p>Note that we are running <code>sbt</code> without any prefix paths because SBT is installed system-wide on your cluster. Also we submit SimpleApp to the real Spark cluster instead of local mode using <code>--master spark://master:7077</code> option.</p>

<h2>Data exploration using Spark SQL</h2>

<p>Next, move to the next page of the Databricks tutorial and try running some <a href="https://databricks-training.s3.amazonaws.com/data-exploration-using-spark-sql.html">Spark SQL examples</a>. We will again need to do some tweaks to run examples from Databrick tutorial on our cluster. Namely, Spark SQL section of this tutorial uses input data stored in <code>spark-training/data</code> folder on local disk, but we are going to run on a real cluster, so we should put data to HDFS:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# cd /root/spark-training
</span><span class='line'>root@master:~# hadoop fs -put data /root/data</span></code></pre></td></tr></table></div></figure>


<p>Now you can run all examples from above mentioned <a href="https://databricks-training.s3.amazonaws.com/data-exploration-using-spark-sql.html">Spark SQL</a> section of Databricks Spark Training tutorial without modifications.</p>

<h2>Stream processing with Spark Streaming</h2>

<p>To run examples from <a href="https://databricks-training.s3.amazonaws.com/realtime-processing-with-spark-streaming.html">Spark Streaming</a> section of the tutorial we again need to do only few modifications. To compile and submit your Spark Streaming application use following commands instead of those suggested in the tutorial:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# sbt assembly
</span><span class='line'>root@master:~# /opt/spark/bin/spark-submit --master spark://master:7077 \ 
</span><span class='line'>--class "Tutorial" \
</span><span class='line'>--master spark://master:7077 target/scala-2.10/Tutorial-assembly-0.1-SNAPSHOT.jar</span></code></pre></td></tr></table></div></figure>


<p>All other contents of Spark Streaming example can be run without modifications on our cluster.</p>

<h2>Movie Recommendation with MLlib</h2>

<p>This section explains how to use MLlib to make personalized movie recommendations. Please read <a href="https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html">detailed description</a> of this example in the original Spark Training tutorial.</p>

<p>To run MLlib example on our cluster on SoftLayer we again have to make some modifications.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~# cd /root/spark-training/machine-learning</span></code></pre></td></tr></table></div></figure>


<p>To make personalized movie recommendations, you should first provide your personal ratings for a few movies as described <a href="https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html#create-training-examples">here</a>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~/spark-training/machine-learning# python bin/rateMovies</span></code></pre></td></tr></table></div></figure>


<p>This script will ask you to provide numerical rating to a set of movies and write result to a file <code>personalRatings.txt</code>.</p>

<p>Move on to the directory where Scala source code for this Movie rating algorithm is stored:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~/spark-training/machine-learning# cd scala</span></code></pre></td></tr></table></div></figure>


<p>Our cluster is running Spark 1.1.0 on top of CDH 5, so original <code>build.sbt</code> file should be modified and look like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import AssemblyKeys._
</span><span class='line'>
</span><span class='line'>assemblySettings
</span><span class='line'>
</span><span class='line'>name := "movielens-als"
</span><span class='line'>
</span><span class='line'>version := "0.1"
</span><span class='line'>
</span><span class='line'>scalaVersion := "2.10.4"
</span><span class='line'>
</span><span class='line'>libraryDependencies += "org.apache.spark" %% "spark-core" % "1.1.0" % "provided"
</span><span class='line'>
</span><span class='line'>libraryDependencies += "org.apache.spark" %% "spark-mllib" % "1.1.0" % "provided"
</span><span class='line'>
</span><span class='line'>libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.3.0" % "provided"
</span><span class='line'>
</span><span class='line'>libraryDependencies += "org.apache.hadoop" % "hadoop-hdfs" % "2.3.0" % "provided"</span></code></pre></td></tr></table></div></figure>


<p>You can download this file to your cluster using following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>wget htts://TODO-TODO/build.sbt</span></code></pre></td></tr></table></div></figure>


<p>After updating <code>build.sbt</code> program can be compiled with SBT assembly command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~/spark-training/machine-learning/scala# sbt assembly</span></code></pre></td></tr></table></div></figure>


<p>To run assembled Spark application use following parameters for <code>spark-submit</code> script:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@master:~/spark-training/machine-learning/scala# /opt/spark/bin/spark-submit \
</span><span class='line'>--master spark://master:7077 \
</span><span class='line'>--class "MovieLensALS" \
</span><span class='line'>target/scala-2.10/movielens-als-assembly-0.1.jar \
</span><span class='line'>hdfs://master:8020/movielens/large ../personalRatings.txt</span></code></pre></td></tr></table></div></figure>


<p>Note that in this command we provide Spark master address <code>spark://master:7077</code>, &ldquo;MovieLensALS&rdquo; class name and assembly jar and two command line parameters for MovieLensALS class: path to <code>movielens/large</code> dataset and path to <code>personalRatings.txt</code> file.</p>

<p>Because you are now running your program on the real Spark cluster, input dataset should be either stored on local file system accessible by all nodes at the same path, or on HDFS. Cluster provisioning script already has preloaded <code>movielens/medium</code> and <code>movielens/large</code> datasets on HDFS of your cluster, so you should provide dataset path as <code>hdfs://master:8020/movielens/medium/</code> or <code>hdfs://master:8020/movielens/large/</code>. Note trailing <code>/</code> at the end of HDFS urls, this is very important for correct constructing of path in <code>MovieLensALS.scala</code>.</p>

<p>Finally, <code>MovieLensALS.scala</code> program is desisgned to read input file from local filesystem, so you should modify a couple of lines in <code>MovieLensALS.scala</code> to read files from HDFS.
Replace line</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val ratings = sc.textFile(new File(movieLensHomeDir, "ratings.dat").toString).map { line =&gt;</span></code></pre></td></tr></table></div></figure>


<p>
by</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val ratings = sc.textFile(movieLensHomeDir + "ratings.dat").map { line =&gt;</span></code></pre></td></tr></table></div></figure>


<p>and do the same with <code>val movies = ...</code> line so it looks like</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val movies = sc.textFile(movieLensHomeDir + "movies.dat").map { line =&gt;</span></code></pre></td></tr></table></div></figure>


<p>Here we simply replaced <code>new File(movieLensHomeDir, "ratings.dat").toString</code> construct by <code>movieLensHomeDir + "ratings.dat"</code>, because <code>new File(...)</code> creates a local File instance from a parent and child path strings, but we do not need this because we provided full HDFS link as a parameter to <code>spark-submit</code>.</p>

<p>Remaining contents of <a href="https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html">MLlib example</a> does not require any modifications for running on a real cluster.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/vagrant-cluster/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/vagrant-cluster/blog/2014/10/27/running-spark-training-examples/">Running Examples From Databricks Spark Tutorial</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Irina Fedulova -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
